{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import json\n",
    "\n",
    "# collect sentences\n",
    "with open(\"dataset_jokes/wocka.json\") as fn:\n",
    "  jokes = json.load(fn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "jokes[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'body': 'What do you call a cow with no legs?\\r\\n\\r\\nGround Beef!',\n",
       " 'category': 'Animal',\n",
       " 'id': 1,\n",
       " 'title': 'Cow With No Legs'}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "sentences = [] # collect sentences\n",
    "\n",
    "for i in jokes: # iterate over all recipes\n",
    "    try:\n",
    "        title = i['title'] # get the title\n",
    "        category = i['category'] # get the category\n",
    "        body = i['body'] # get the body\n",
    "        sentence = f\"{title}, {category}, {body}\" # create the sentence as string\n",
    "        if sentence != '': # if the sentence is not empty\n",
    "            sentences.append(sentence) # add the sentence to the list\n",
    "    except KeyError: # if the recipe has no title or ingredients\n",
    "        continue\n",
    "\n",
    "# clean sentences\n",
    "# TODO: add further cleaning steps\n",
    "def clean(sentence):\n",
    "    sentence = sentence.replace('\\r', ' ')  # replace repetetive words\n",
    "    sentence = sentence.replace('\\n', '')  # replace new line chars\n",
    "    sentence = sentence.replace('  ', ' ')  # replace repetetive words\n",
    "    sentence = sentence.strip()  # strip leading and trailing white-spaces\n",
    "    return sentence\n",
    "\n",
    "sentences = list(map(clean, sentences))  # map method.\n",
    "# sentences = [clean(sentence) for sentence in sentences]  # list comprehension method"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "sentences[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Cow With No Legs, Animal, What do you call a cow with no legs? Ground Beef!'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into train/dev\n",
    "# TODO: alternatively, we could use the `datasets.Dataset.train_test_split()` method \n",
    "SEED = 10  # set seed var for reproducibility\n",
    "train_sentences, test_sentences = train_test_split(sentences, \n",
    "                                                   test_size=0.1, \n",
    "                                                   # change the train_size for rapid testing (for example, use 0.1)\n",
    "                                                   train_size=0.9,  \n",
    "                                                   random_state=SEED)\n",
    "\n",
    "# write into files\n",
    "for split, sents in zip(['train', 'test'], [train_sentences, test_sentences]):\n",
    "    with open(f\"{split}.txt\", 'w') as fn:\n",
    "        fn.write('\\n'.join(sents))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# create the datasets.Dataset object\n",
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset('text', data_files={'train': 'train.txt', 'test': 'test.txt'}) # load the dataset from the text files"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-6a30f5af75f6bfd4\n",
      "                            "
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/deniz/.cache/huggingface/datasets/text/default-6a30f5af75f6bfd4/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
      "Dataset text downloaded and prepared to /home/deniz/.cache/huggingface/datasets/text/default-6a30f5af75f6bfd4/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 9052\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1002\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Instantiate tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "pretrained_model = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=pretrained_model)\n",
    "\n",
    "# Define a function to tokenize the dataset and return the text indices. \n",
    "# We also add trailing <|endoftext|> special token\n",
    "def tokenize_sentence(dataset):\n",
    "    # As we can see, there is no padding since the PAD token is not originally used by GPT-2. \n",
    "    # We could perform padding by adding the PAD token to the vocabulary with the method `add_special_tokens()`\n",
    "    return tokenizer([f\"{sentence} {tokenizer.eos_token}\" for sentence in dataset['text']])\n",
    "    # return tokenizer(dataset['text])\n",
    "\n",
    "# apply to dataset object\n",
    "dataset_features = dataset.map(tokenize_sentence,\n",
    "                               batched=True,\n",
    "                               remove_columns=['text'],\n",
    "                               desc='Tokenizing train and test splits')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Tokenizing train and test splits:   0%|          | 0/10 [00:00<?, ?ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1127 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing train and test splits: 100%|██████████| 10/10 [00:00<00:00, 14.49ba/s]\n",
      "Tokenizing train and test splits: 100%|██████████| 2/2 [00:00<00:00, 23.41ba/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "dataset_features"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids'],\n",
       "        num_rows: 9052\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids'],\n",
       "        num_rows: 1002\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# group sentences in batches of equal size (standard GPT-2 approach)\n",
    "# We use an adaptation of the `group_text` function for that purpose\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    block_size = 512  # set the \"blocks\" to half of the maximum GPT-2 model length (1024) for memory issues\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "\n",
    "    # # Add labels to the dataset_features\n",
    "    # # Since the task is language modelling, the labels to predict are actually the input indices \"shifted\"\n",
    "\n",
    "    # result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# apply the group function to the dataset\n",
    "\n",
    "dataset_grouped = dataset_features.map(group_texts,\n",
    "                                       batched=True,\n",
    "                                       desc='Group sentences in blocks of equal size (512)')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Group sentences in blocks of equal size (512): 100%|██████████| 10/10 [00:03<00:00,  2.60ba/s]\n",
      "Group sentences in blocks of equal size (512): 100%|██████████| 2/2 [00:00<00:00,  5.22ba/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check block size "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in dataset_grouped['train']['input_ids']:\n",
    "    if len(i) != 512:\n",
    "        print(len(i))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "for i in dataset_grouped['test']['input_ids']:\n",
    "    if len(i) != 512:\n",
    "        print(i)\n",
    "        print(len(i))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[4834, 15521, 972, 11, 8366, 11, 4874, 612, 373, 257, 2576, 508, 2227, 4025, 17515, 11, 523, 530, 1110, 673, 1816, 284, 766, 607, 6253, 11, 1583, 13, 4176, 13, 220, 1583, 13, 4176, 1297, 607, 284, 6437, 607, 17515, 290, 9585, 262, 1708, 25, 366, 6173, 6684, 3483, 36, 11, 35, 6684, 3483, 36, 11, 43, 6684, 3483, 36, 11, 314, 41300, 26746, 30373, 347, 6684, 3483, 1546, 1911, 1881, 1110, 673, 373, 2491, 2739, 11, 290, 3066, 284, 466, 607, 13565, 319, 262, 1323, 618, 257, 3516, 1625, 510, 284, 607, 290, 1965, 611, 673, 373, 257, 5827, 286, 1583, 13, 4176, 338, 11, 284, 543, 673, 8712, 25, 366, 5297, 11, 703, 750, 345, 760, 43634, 679, 8712, 366, 39, 11860, 15513, 360, 11860, 15513, 37760, 2474, 220, 50256, 38101, 29926, 2611, 1406, 12301, 11, 25455, 337, 2002, 64, 11, 25455, 40084, 523, 3735, 326, 673, 17157, 625, 290, 1392, 5169, 329, 6301, 8469, 13, 220, 50256]\n",
      "160\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# Add \"labels\" column to the dataset_features. \n",
    "# To modify the dataset structure, we use the `dataset.map()` method\n",
    "def add_labels(dataset):\n",
    "    # Since the task is language modelling, the labels to predict are actually \n",
    "    # the input indices shifted forward by one element (token)\n",
    "    dataset['labels'] = dataset['input_ids'].copy()\n",
    "    return dataset\n",
    "\n",
    "dataset_for_lm = dataset_grouped.map(add_labels,\n",
    "                                     batched=True,\n",
    "                                     desc='Add labels to create data for language model training')\n",
    " "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Add labels to create data for language model training: 100%|██████████| 3/3 [00:00<00:00,  4.11ba/s]\n",
      "Add labels to create data for language model training: 100%|██████████| 1/1 [00:00<00:00, 14.45ba/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# Instantiate the model class\n",
    "from transformers import (\n",
    "    AutoConfig, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "import torch\n",
    "\n",
    "\n",
    "# TODO: experiment with different model configuration and batch sizes until \n",
    "# the models fits into GPU memory (otherwise it generated CUDA-out-of-memory error)\n",
    "# The model is instantiated from the pretrained GPT-2 model\n",
    "# Here, I reduced the number of attention head and layers, \n",
    "# to significantly reduce the model size and make sure it fits in the GPU memory\n",
    "config = AutoConfig.from_pretrained(pretrained_model,\n",
    "                                    n_head=12,  # reduce the size of the model for memory issues\n",
    "                                    n_layer=12)\n",
    "\n",
    "pretrained_model = 'gpt2-recipes'\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model, \n",
    "                                             config=config)\n",
    "\n",
    "# Again, we simulate a batch size of 8 by setting the `gradient_accumulation_steps` parameters\n",
    "no_cuda = not bool(torch.cuda.is_available())\n",
    "\n",
    "if no_cuda:\n",
    "  print(f\"Training on CPUs\")\n",
    "else:\n",
    "  print(f\"Training on GPU\")\n",
    "\n",
    "training_args = TrainingArguments(no_cuda=no_cuda,\n",
    "                                  per_device_train_batch_size=4,\n",
    "                                  per_device_eval_batch_size=4,\n",
    "                                  gradient_accumulation_steps=4, # virtually increment the batch_size\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  save_strategy='epoch',\n",
    "                                  logging_steps=100,\n",
    "                                  logging_dir='gpt2-jokes/tb',  # where to store the tensorboard\n",
    "                                  num_train_epochs=10,\n",
    "                                  output_dir='gpt2-jokes')\n",
    "\n",
    "# Start the training!\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_for_lm['train'],\n",
    "    eval_dataset=dataset_for_lm['test'], # we use the test set as validation set\n",
    "    tokenizer=tokenizer,\n",
    "    # Data collator is used to create batches from data. \n",
    "    # When a tokenizer is passed the default to DataCollatorWithPadding is used.\n",
    "    # So we change it since our model do not use PAD tokens\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file gpt2-recipes/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file gpt2-recipes/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-recipes.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# Use tensorboard to monitor the training\n",
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard  \n",
    "\n",
    " # read data from tensorboard dir\n",
    "%tensorboard --logdir gpt2-jokes/tb "
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# Finally: let's start the training!\n",
    "train_results = trainer.train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2802\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1750\n",
      "  6%|▌         | 100/1750 [02:06<33:49,  1.23s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 3.4358, 'learning_rate': 4.714285714285714e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 10%|█         | 175/1750 [03:38<31:53,  1.21s/it]***** Running Evaluation *****\n",
      "  Num examples = 285\n",
      "  Batch size = 4\n",
      "\n",
      " 10%|█         | 175/1750 [03:46<31:53,  1.21s/it]Saving model checkpoint to gpt2-jokes/checkpoint-175\n",
      "Configuration saved in gpt2-jokes/checkpoint-175/config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 3.1719183921813965, 'eval_runtime': 7.4121, 'eval_samples_per_second': 38.451, 'eval_steps_per_second': 9.714, 'epoch': 1.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in gpt2-jokes/checkpoint-175/pytorch_model.bin\n",
      "tokenizer config file saved in gpt2-jokes/checkpoint-175/tokenizer_config.json\n",
      "Special tokens file saved in gpt2-jokes/checkpoint-175/special_tokens_map.json\n",
      " 11%|█▏        | 200/1750 [04:18<31:37,  1.22s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 3.2798, 'learning_rate': 4.428571428571428e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 17%|█▋        | 300/1750 [06:20<29:30,  1.22s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 3.1755, 'learning_rate': 4.1428571428571437e-05, 'epoch': 1.71}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 20%|██        | 350/1750 [07:21<28:58,  1.24s/it]***** Running Evaluation *****\n",
      "  Num examples = 285\n",
      "  Batch size = 4\n",
      "\n",
      " 20%|██        | 350/1750 [07:29<28:58,  1.24s/it]Saving model checkpoint to gpt2-jokes/checkpoint-350\n",
      "Configuration saved in gpt2-jokes/checkpoint-350/config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 3.120232343673706, 'eval_runtime': 7.4185, 'eval_samples_per_second': 38.417, 'eval_steps_per_second': 9.705, 'epoch': 2.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in gpt2-jokes/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in gpt2-jokes/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in gpt2-jokes/checkpoint-350/special_tokens_map.json\n",
      " 23%|██▎       | 400/1750 [08:32<27:59,  1.24s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 3.1372, 'learning_rate': 3.857142857142858e-05, 'epoch': 2.29}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 29%|██▊       | 500/1750 [10:37<27:13,  1.31s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 3.0679, 'learning_rate': 3.571428571428572e-05, 'epoch': 2.86}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 30%|███       | 525/1750 [11:09<26:02,  1.28s/it]***** Running Evaluation *****\n",
      "  Num examples = 285\n",
      "  Batch size = 4\n",
      "\n",
      " 30%|███       | 525/1750 [11:17<26:02,  1.28s/it]Saving model checkpoint to gpt2-jokes/checkpoint-525\n",
      "Configuration saved in gpt2-jokes/checkpoint-525/config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 3.0964958667755127, 'eval_runtime': 8.2067, 'eval_samples_per_second': 34.728, 'eval_steps_per_second': 8.773, 'epoch': 3.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in gpt2-jokes/checkpoint-525/pytorch_model.bin\n",
      "tokenizer config file saved in gpt2-jokes/checkpoint-525/tokenizer_config.json\n",
      "Special tokens file saved in gpt2-jokes/checkpoint-525/special_tokens_map.json\n",
      " 34%|███▍      | 600/1750 [12:56<24:36,  1.28s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 3.0294, 'learning_rate': 3.285714285714286e-05, 'epoch': 3.43}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 40%|████      | 700/1750 [15:06<22:39,  1.30s/it]***** Running Evaluation *****\n",
      "  Num examples = 285\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 3.0154, 'learning_rate': 3e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      " 40%|████      | 700/1750 [15:14<22:39,  1.30s/it]Saving model checkpoint to gpt2-jokes/checkpoint-700\n",
      "Configuration saved in gpt2-jokes/checkpoint-700/config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 3.0854358673095703, 'eval_runtime': 7.962, 'eval_samples_per_second': 35.795, 'eval_steps_per_second': 9.043, 'epoch': 4.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in gpt2-jokes/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in gpt2-jokes/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in gpt2-jokes/checkpoint-700/special_tokens_map.json\n",
      " 46%|████▌     | 800/1750 [17:23<19:17,  1.22s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 2.9664, 'learning_rate': 2.714285714285714e-05, 'epoch': 4.57}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 50%|█████     | 875/1750 [18:55<17:56,  1.23s/it]***** Running Evaluation *****\n",
      "  Num examples = 285\n",
      "  Batch size = 4\n",
      "\n",
      " 50%|█████     | 875/1750 [19:04<17:56,  1.23s/it]Saving model checkpoint to gpt2-jokes/checkpoint-875\n",
      "Configuration saved in gpt2-jokes/checkpoint-875/config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 3.0803442001342773, 'eval_runtime': 8.2368, 'eval_samples_per_second': 34.601, 'eval_steps_per_second': 8.741, 'epoch': 5.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in gpt2-jokes/checkpoint-875/pytorch_model.bin\n",
      "tokenizer config file saved in gpt2-jokes/checkpoint-875/tokenizer_config.json\n",
      "Special tokens file saved in gpt2-jokes/checkpoint-875/special_tokens_map.json\n",
      " 51%|█████▏    | 900/1750 [19:37<17:57,  1.27s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 2.9507, 'learning_rate': 2.4285714285714288e-05, 'epoch': 5.14}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 57%|█████▋    | 1000/1750 [21:40<15:10,  1.21s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 2.9288, 'learning_rate': 2.1428571428571428e-05, 'epoch': 5.71}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 60%|██████    | 1050/1750 [22:41<14:11,  1.22s/it]***** Running Evaluation *****\n",
      "  Num examples = 285\n",
      "  Batch size = 4\n",
      "\n",
      " 60%|██████    | 1050/1750 [22:49<14:11,  1.22s/it]Saving model checkpoint to gpt2-jokes/checkpoint-1050\n",
      "Configuration saved in gpt2-jokes/checkpoint-1050/config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 3.074277877807617, 'eval_runtime': 8.2019, 'eval_samples_per_second': 34.748, 'eval_steps_per_second': 8.778, 'epoch': 6.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in gpt2-jokes/checkpoint-1050/pytorch_model.bin\n",
      "tokenizer config file saved in gpt2-jokes/checkpoint-1050/tokenizer_config.json\n",
      "Special tokens file saved in gpt2-jokes/checkpoint-1050/special_tokens_map.json\n",
      " 63%|██████▎   | 1100/1750 [23:52<13:32,  1.25s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 2.9012, 'learning_rate': 1.8571428571428572e-05, 'epoch': 6.29}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 69%|██████▊   | 1200/1750 [25:56<10:53,  1.19s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 2.8806, 'learning_rate': 1.5714285714285715e-05, 'epoch': 6.86}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 70%|███████   | 1225/1750 [26:26<10:22,  1.19s/it]***** Running Evaluation *****\n",
      "  Num examples = 285\n",
      "  Batch size = 4\n",
      "\n",
      " 70%|███████   | 1225/1750 [26:34<10:22,  1.19s/it]Saving model checkpoint to gpt2-jokes/checkpoint-1225\n",
      "Configuration saved in gpt2-jokes/checkpoint-1225/config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 3.073920965194702, 'eval_runtime': 8.0765, 'eval_samples_per_second': 35.288, 'eval_steps_per_second': 8.915, 'epoch': 7.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in gpt2-jokes/checkpoint-1225/pytorch_model.bin\n",
      "tokenizer config file saved in gpt2-jokes/checkpoint-1225/tokenizer_config.json\n",
      "Special tokens file saved in gpt2-jokes/checkpoint-1225/special_tokens_map.json\n",
      " 74%|███████▍  | 1300/1750 [28:06<08:53,  1.19s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 2.8783, 'learning_rate': 1.2857142857142857e-05, 'epoch': 7.43}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 80%|████████  | 1400/1750 [30:04<06:55,  1.19s/it]***** Running Evaluation *****\n",
      "  Num examples = 285\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 2.8589, 'learning_rate': 1e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      " 80%|████████  | 1400/1750 [30:13<06:55,  1.19s/it]Saving model checkpoint to gpt2-jokes/checkpoint-1400\n",
      "Configuration saved in gpt2-jokes/checkpoint-1400/config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 3.0733442306518555, 'eval_runtime': 8.6275, 'eval_samples_per_second': 33.034, 'eval_steps_per_second': 8.345, 'epoch': 8.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in gpt2-jokes/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in gpt2-jokes/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in gpt2-jokes/checkpoint-1400/special_tokens_map.json\n",
      " 86%|████████▌ | 1500/1750 [32:14<04:56,  1.19s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 2.85, 'learning_rate': 7.142857142857143e-06, 'epoch': 8.57}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 90%|█████████ | 1575/1750 [33:43<03:27,  1.19s/it]***** Running Evaluation *****\n",
      "  Num examples = 285\n",
      "  Batch size = 4\n",
      "\n",
      " 90%|█████████ | 1575/1750 [33:52<03:27,  1.19s/it]Saving model checkpoint to gpt2-jokes/checkpoint-1575\n",
      "Configuration saved in gpt2-jokes/checkpoint-1575/config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 3.073152542114258, 'eval_runtime': 8.8302, 'eval_samples_per_second': 32.276, 'eval_steps_per_second': 8.154, 'epoch': 9.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in gpt2-jokes/checkpoint-1575/pytorch_model.bin\n",
      "tokenizer config file saved in gpt2-jokes/checkpoint-1575/tokenizer_config.json\n",
      "Special tokens file saved in gpt2-jokes/checkpoint-1575/special_tokens_map.json\n",
      " 91%|█████████▏| 1600/1750 [34:24<03:02,  1.22s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 2.8406, 'learning_rate': 4.285714285714286e-06, 'epoch': 9.14}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 97%|█████████▋| 1700/1750 [36:27<00:59,  1.19s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 2.8356, 'learning_rate': 1.4285714285714286e-06, 'epoch': 9.71}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1750/1750 [37:27<00:00,  1.18s/it]***** Running Evaluation *****\n",
      "  Num examples = 285\n",
      "  Batch size = 4\n",
      "\n",
      "100%|██████████| 1750/1750 [37:36<00:00,  1.18s/it]Saving model checkpoint to gpt2-jokes/checkpoint-1750\n",
      "Configuration saved in gpt2-jokes/checkpoint-1750/config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 3.0730676651000977, 'eval_runtime': 8.8754, 'eval_samples_per_second': 32.111, 'eval_steps_per_second': 8.112, 'epoch': 10.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in gpt2-jokes/checkpoint-1750/pytorch_model.bin\n",
      "tokenizer config file saved in gpt2-jokes/checkpoint-1750/tokenizer_config.json\n",
      "Special tokens file saved in gpt2-jokes/checkpoint-1750/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 1750/1750 [37:38<00:00,  1.29s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'train_runtime': 2258.2127, 'train_samples_per_second': 12.408, 'train_steps_per_second': 0.775, 'train_loss': 2.9967728445870536, 'epoch': 10.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "# Save model and tokenizer\n",
    "trainer.save_model('gpt2-jokes')\n",
    "\n",
    "# Save the metrics obtained (loss)\n",
    "metrics_train = train_results.metrics\n",
    "trainer.log_metrics('train', metrics_train)\n",
    "trainer.save_metrics('train', metrics_train)\n",
    "\n",
    "# save trainer state Saves the Trainer state, since Trainer.save_model \n",
    "# saves only the tokenizer with the model\n",
    "trainer.save_state()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Saving model checkpoint to gpt2-jokes\n",
      "Configuration saved in gpt2-jokes/config.json\n",
      "Model weights saved in gpt2-jokes/pytorch_model.bin\n",
      "tokenizer config file saved in gpt2-jokes/tokenizer_config.json\n",
      "Special tokens file saved in gpt2-jokes/special_tokens_map.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  train_loss               =     2.9968\n",
      "  train_runtime            = 0:37:38.21\n",
      "  train_samples_per_second =     12.408\n",
      "  train_steps_per_second   =      0.775\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Evaluate the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "metrics_eval = trainer.evaluate()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 285\n",
      "  Batch size = 4\n",
      "100%|██████████| 72/72 [00:07<00:00,  9.02it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "import math\n",
    "\n",
    "# compute perplexity as the exponential of the loss (cross-entropy)\n",
    "perplexity = math.exp(metrics_eval['eval_loss'])\n",
    "metrics_eval['perplexity'] = perplexity\n",
    "\n",
    "# save evaluation metrics\n",
    "trainer.log_metrics('eval', metrics_eval)\n",
    "trainer.save_metrics('eval', metrics_eval)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_loss               =     3.1606\n",
      "  eval_runtime            = 0:00:07.99\n",
      "  eval_samples_per_second =     35.641\n",
      "  eval_steps_per_second   =      9.004\n",
      "  perplexity              =    23.5845\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "from transformers import TextGenerationPipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = 'gpt2-jokes'\n",
    "model_checkpoint = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "pipeline_generate = TextGenerationPipeline(model_checkpoint,\n",
    "                                           tokenizer=tokenizer)\n",
    "\n",
    "while True:\n",
    "    prompt = input('\\n\\nInsert prompt\\n')\n",
    "    max_length = int(input('\\nInsert max generation length\\n'))\n",
    "    top_p = float(input('\\nInsert top_p\\n'))\n",
    "    top_k = int(input('\\nInsert top_k\\n'))\n",
    "    num_return_sequences = int(input('\\nInsert num_return_sequences\\n'))\n",
    "    \n",
    "    generated_sentence = pipeline_generate(prompt,    \n",
    "                                           max_length=max_length,\n",
    "                                           do_sample=True,\n",
    "                                           top_k=top_k,\n",
    "                                           top_p=top_p,\n",
    "                                           num_return_sequences=num_return_sequences,\n",
    "                                           early_stopping=False)\n",
    "    \n",
    "    \n",
    "    for gen in generated_sentence:\n",
    "        print(gen['generated_text'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file gpt2-jokes/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2-recipes\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file gpt2-jokes/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-jokes.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file gpt2-jokes/added_tokens.json. We won't load it.\n",
      "loading file gpt2-jokes/vocab.json\n",
      "loading file gpt2-jokes/merges.txt\n",
      "loading file gpt2-jokes/tokenizer.json\n",
      "loading file None\n",
      "loading file gpt2-jokes/special_tokens_map.json\n",
      "loading file gpt2-jokes/tokenizer_config.json\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "two guys walk in a bar. The first guy asks the second guy if he's ever been to a bar before. The second guy says, \"Yes, I've been to a bar before but I haven't ever seen someone drinking.\" So the\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "two guys walk in a bar  and one guy yells \"Sell to the people on the ground\" \n",
      "two guys walk in a bar \n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-410c00bc5143>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\nInsert prompt\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nInsert max generation length\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nInsert top_p\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nInsert top_k\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('NLP': conda)"
  },
  "interpreter": {
   "hash": "86c899ea07089cc9d284ea9f9b0758fa4e79f644b216f9a03b961d7e004425f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}